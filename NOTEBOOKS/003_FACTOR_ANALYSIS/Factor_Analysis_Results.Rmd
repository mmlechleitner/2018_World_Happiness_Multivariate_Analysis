---
title: "Factor Analysis on World Dataset"
author: "Daniel Cooper, Maria Lechleitner, Dahmane Sheikh"
date: "April 28th 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Introduction:
We will perform Factor Analysis (FA) of the world happiness dataset to reduce dimensionality of the input data. The idea is to identify the unobserved, underlying factors of the data that can explain the intercorrelation among the variables. 

### Variable Selection

The input data was scaled using the function BestNormalize to optimize scaling for each variable before computing the correlation coefficients that are used for the analysis. The correlation matrix instead of the covariance-variance matrix is used here because of the different scaling of variables. However, we could also use the covariance-variance matrix here, as the data is normalized before performing FA.

```{r, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
#### Setup ####
# Required Libraries:
library(tidyverse)
library(skimr)
library(data.table)
library(magrittr)
library(sos)
library(brew)
library(matrixcalc)
library(psych)

#Load dataset
happiness_normal <- read.csv("C:/Users/mmlec/Desktop/2018_World_Happiness_Multivariate_Analysis/DATA/full_best_df.csv")

#Selecting only numeric values with KMO>0.6 and Loadings>0.5 
selected <- happiness_normal %>%
  select(-X,
         -Country,
         -Region,
         -Happiness_Score,
         -Total_Population,
         -Mental_and_Substance_Disorder_Index,
         -Compulsory_Education_in_Years,
         -Agricultural_Land_Percentage,
         -Generosity,
         #-Infant_Immunization_Measles_Index,
         -Forest_Area_Land_Percentage,
         -Suicide_Index,
         -Legal_Rights_Index)
selected<-data.matrix(selected)

```

### Check if data is appropriate for FA

First, we want to check if the data is appropriate for factor analysis. It is appropriate either if there are
- high correleations among the variables or
- low partial correleation among the variables.

### Correlation Matrix

We are computing the Correlation Matrix to see if the variables can be transformed into homogeneous groups. High correlation values indicate that the variables have much in common.

```{r, echo = TRUE, fig.width=8, fig.height=8}
R<-cor(selected)

```

### Partial Correlation Matrix

The Partial correlations measures the linear relationship between two variables after removing the effects of the remaining variables. We want low linear relationship, meaning that the variables have no spurious correlations among each other.

```{r, echo = TRUE, fig.width=8, fig.height=8}
S<-matrix(rep(0,(dim(R)[2]^2)),nrow=dim(R)[2])
diag(S)<-sqrt(diag(solve(R))^(-1))

Q<-S%*%solve(R)%*%S # partial correlation matrix

sum_q2<-sum(lower.triangle(Q)^2)-sum(diag(Q)^2)
sum_r2<-sum(lower.triangle(R)^2)-sum(diag(R)^2)

```

### KMO

Kaiser-Meyer-Olkin Measure of Sampling Adequacy
The method compares the squared elements of the partial correlation matrix to the squares of the original correlations. The Measure of Sampling Adequacy (MSA) should be as close to 1 as possible.

```{r, echo = TRUE, fig.width=8, fig.height=8}
###First KMO
#KMO1<-1-sum_q2/sum_r2
###Second KMO
#KMO2<-sum_r2/(sum_r2+sum_q2)

#Use of Function KMO, no manual computation needed
KMO(R) 

```

The overall MSA of our data is 0.92, which indicates a very good data quality in terms of performing Factor Analysis. It means that our variables are highly correlated and that the partial correlations are low.

### PCA to obtain number of factors

PCA is performed to extract the number of factors. Question to be asked: How many common factors are to be included in the model?

This requires a determination of how many parameters are going to be involved.

### Number of unique parameters

Formula to caluclate the number of parameters that the correlation matrix contains: p(p+1)/2

```{r, echo = TRUE, fig.width=8, fig.height=8}
#number of unique elements in R = 210
20*21/2

```

### Number of parameters in the factor model with m factors

Formula to caluclate the number of parameters in the factor model: mp + p


```{r, echo = TRUE, fig.width=8, fig.height=8}
# number of parameters of the linear model with 4 factors = 100 - already a big reduction
4*20+20

# number of parameters of the linear model with 3 factors = 80 - num is not decreasing so much
3*20+20

# number of parameters of the linear model with 2 factors = 60 - num is not decreasing so much
2*20+20

```

This would result in a significant dimension reduction.
In this case, we would select m = 3, yielding 80 parameters in the factor model.

However, lets talk a look on the results of PCA.

### Eigenvalues, Eigenvectors

We compute the Eigenvalues and Eigenvectors of the correlation matrix of our smaple as a basis for PCA analysis.

```{r, echo = TRUE, fig.width=8, fig.height=8, warning = FALSE, message = FALSE}
#Eigenvalues and Eigenvectors
eigen(R)
plot(eigen(R)$values, type="b")

```

### Examine Eigenvalues

We examine the eigenvalues to determine the number of principal compenents being considered:

```{r, echo = TRUE, fig.width=8, fig.height=8, warning = FALSE, message = FALSE}
perc_explained<-eigen(R)$values/20
cum_explain<-cumsum(perc_explained)
table<-cbind(eigenvalue=eigen(R)$values,perc_explained,cum_explain)
table
rm(table)

```

From the table above you can see, that the first 3 components explain 72% of the variation. Taking into account our problem, we consider this as a sufficient representation of our data.

```{r, echo = TRUE, fig.width=8, fig.height=8, warning = FALSE, message = FALSE}
#Check the sum of eigenvalues (should be equal to the number of variables)
sum(eigen(R)$values)

```

### Loadings

The loadings show the correlation between factors and original variables. This will be used to interpret the factors. We want to optain highly correlated combinations, so the cut-off critierion is 0.5. If a variable is below the given threashold, we simply exclude it from our analysis.

```{r, echo = TRUE, fig.width=8, fig.height=8}
D<-matrix(rep(0,(20*20)),nrow=20)
diag(D)<-sqrt(eigen(R)$values)
loadings<-eigen(R)$vectors%*%D
rownames(loadings)<-colnames(R[,1:20])
loadings[,1:3] #number of factors obtained
#cov(loadings) #are factors uncorrelated with each other ???

```

```{r, echo = TRUE, fig.width=8, fig.height=8}
 #Biplot of PC1 and PC2
print(res.pca)

```

```{r, echo = TRUE, fig.width=8, fig.height=8}
res.pca <- PCA(selected)
fviz_eig(res.pca, addlabels=TRUE) #Scree plot

```

### Communalities

The Communality is a measure for how well the model performs for a particular variable. The larger the communality the better the model performance for the j-th indicator. 

```{r, echo = TRUE, fig.width=8, fig.height=8}
Communalities<-matrix(rep(0,21),nrow=21)
Communalities[1]<-sum(loadings[1,1:3]^2)
Communalities[2]<-sum(loadings[2,1:3]^2)
Communalities[3]<-sum(loadings[3,1:3]^2)
Communalities[4]<-sum(loadings[4,1:3]^2)
Communalities[5]<-sum(loadings[5,1:3]^2)
Communalities[6]<-sum(loadings[6,1:3]^2)
Communalities[7]<-sum(loadings[7,1:3]^2)
Communalities[8]<-sum(loadings[8,1:3]^2)
Communalities[9]<-sum(loadings[9,1:3]^2)
Communalities[10]<-sum(loadings[10,1:3]^2)
Communalities[11]<-sum(loadings[11,1:3]^2)
Communalities[12]<-sum(loadings[12,1:3]^2)
Communalities[13]<-sum(loadings[13,1:3]^2)
Communalities[14]<-sum(loadings[14,1:3]^2)
Communalities[15]<-sum(loadings[15,1:3]^2)
Communalities[16]<-sum(loadings[16,1:3]^2)
Communalities[17]<-sum(loadings[17,1:3]^2)
Communalities[18]<-sum(loadings[18,1:3]^2)
Communalities[19]<-sum(loadings[19,1:3]^2)
Communalities[20]<-sum(loadings[20,1:3]^2)
Communalities[21]<-sum(Communalities)
rownames(Communalities)<-c(dimnames(R)[[1]],"total")
Communalities

sum(eigen(R)$values[1:3]) #Sum of the first 3 eigenvalues

```

These values are like multiple RÂ² values for regression models that predict the variables of intrest from the 3 facotrs.
Here, the communality value for a given variable can be interpreted as the proportion of variation explained by the three factors and thus, explains how well this model is performing.
Being precise, that means that the factor analysis does the best job in explaning variation in Electricity Access, Economy, Internet Usage and Child Mortality. Whereas it can not explain Cellular Subscriber, Obesity and Infant Immunization Measles (not even explaining half of the variation) very well.

The total communality value is 14.39. The proportion of the total variation explained by the three factors is 14.39/20 = 72% (same value as cum_explain and first 3 eigenvalues). Therefore, its an alternative assessment of the model than assessment by eigenvalues.

### Specific variances 

Specific variances are computed by subtracting the communality from the variance. Can be found in the diagonal elements of the table presented below.

```{r, echo = TRUE, fig.width=8, fig.height=8}
Psi<-matrix(rep(0,400),nrow=20)
diag(Psi)<-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)-Communalities[1:20]
Psi

```

### Residuals

The model will have some errors because the it provides only an approximation to the correlation matrix.

```{r, echo = TRUE, fig.width=8, fig.height=8}
#factors that are not explained by the model
Residual_R<-R-loadings[,1:3]%*%t(loadings[,1:3])-Psi
dimnames(Residual_R)<-dimnames(R)
Residual_R

```

Residuals should be as close to 0 as possible. 

### Factor rotation - Varimax

We want to rotate the axis to obtain a cleaner interpretation of the data. 

```{r, echo = TRUE, fig.width=8, fig.height=8}
#LETS IMPROVE THE LOADINGS SOLUTION
varimax(loadings[,1:3])
#Eigenvalues
eigen(R)$values

```
#Eigenvalues remain the same

#PCA
[1] 10.50347943  1.97393591  1.90764707  1.00687127  0.91896263  0.57882411  0.47899389  0.44842127  0.38390587
[10]  0.33826679  0.26176412  0.22852910  0.20439057  0.17796399  0.15061393  0.12456164  0.10684294  0.08761522
[19]  0.07123781  0.04717244
#FA
 [1] 10.50347943  1.97393591  1.90764707  1.00687127  0.91896263  0.57882411  0.47899389  0.44842127  0.38390587
[10]  0.33826679  0.26176412  0.22852910  0.20439057  0.17796399  0.15061393  0.12456164  0.10684294  0.08761522
[19]  0.07123781  0.04717244

sum should be the same

#Loadings change
#FA
                [,1]  [,2]  [,3]
SS loadings    7.690 4.333 2.362
Proportion Var 0.385 0.217 0.118
Cumulative Var 0.385 0.601 0.719

### Interpretation

Factor 1: (Country Development)
- Economy_GDP_Per_Capital (strong)
- Health_Life_Expectancy (strong)
- Internet_Usage_Index (strong)
- Cellular_Subscriber_Index
- Child_Mortality_Index (strong)
- Birth_Rate_Index
- Clean_Water_Index (strong)
- HIV_Disease_Index (strong)
- Obesity_Index
- Electricity_Access_Population_Percentage (strong)
- Entrepreneurship_Cost_Index  
- Infant_Immunization_Measles_Index 
- Automotive_Mortality_Index
- Urban_Population_Percentage (strong)

Factor 2: (Quality of Life)
- Family
- Air_Pollution (strong)
- Alcohol_Consumption_Index (strong)
- Population_Growth_Rate (decreasing)
- Child Mortality (already in FA1)
- Birth-Rate-Index (already in FA1)
- Automotive Mortality Index (already in FA1)

As Factor 2 increases,
- Family decreases
- Air pollution increases
- Alcohol consumption decreases
- ...

The happier the family, the less is the population growth rate and the air pollution. However, the alcohol consumption rises as the happiness grows.

Factor 3: (Freedom)
- Trust_Government_Curruption
- Freedom


##Generate FA Data

```{r, echo = TRUE, fig.width=8, fig.height=8}
# Varimax Rotated Principal Components
# retaining 3 components 
library(psych)
FA_Happiness <- principal(selected, nfactors=3, rotate="varimax")
FA_Happiness$scores[,1]

```


### Plots for Poster

The function "principal" performed bellow substitutes the whole work "manually" done before. ;-)

```{r, echo = TRUE, fig.width=8, fig.height=8}
# Varimax Rotated Principal Components
# retaining 3 components 
library(psych)
fit <- principal(selected, nfactors=3, rotate="varimax")
sum((fit$loadings[,2]^2)) # print Eigenvectors
fit$values #print Eigenvalues
sum(fit$values)

biplot(fit, main = NULL)
fa.diagram(fit)

fa.parallel(selected) #Dont understand this!

```


###Compare Loadings

```{r, echo = TRUE, fig.width=8, fig.height=8}

FA_Loadings<-principal(selected, nfactors=3, rotate="varimax")
FA_Loadings<-fit$loadings # Loadings
rownames<-list(rownames(loadings))
FA_Loadings<-matrix(FA_Loadings, nrow = 20, ncol = 3, dimnames = rownames)
FA_Loadings

D<-matrix(rep(0,(20*20)),nrow=20)
diag(D)<-sqrt(eigen(R)$values)
loadings<-eigen(R)$vectors%*%D
rownames(loadings)<-colnames(R[,1:20])
PCA_Loadings<-loadings[,1:3] #number of factors obtained
PCA_Loadings

ggcorrplot(FA_Loadings, lab=T, insig ="blank") + coord_flip() + ggtitle("FA Loadings")
ggcorrplot(PCA_Loadings, lab=T, insig ="blank") + coord_flip() + ggtitle("PCA Loadings")

biplot(selected, FA_Loadings)

```

### Scree Plot

```{r, echo = TRUE, fig.width=8, fig.height=8}
xaxis <- 1:3
FAeig <- c(sum((fit$loadings[,1]^2)), sum((fit$loadings[,2]^2)), sum((fit$loadings[,3]^2)))
eig <- data.frame(FAeig)
eig <- eig[1:3,]
temp_for_plot <- data.frame(eig,xaxis)
perc_explained_fa<-eig/20
perc_explained_fa

temp_for_plot$percentage.of.variance <- round(perc_explained_fa, digits=4)

ggplot(data=temp_for_plot, aes(x=factor(xaxis), y=temp_for_plot$percentage.of.variance*100)) +
  geom_bar(stat="identity", fill="steelblue") +
    #axistitle:
  geom_text(aes(label= paste0(temp_for_plot$percentage.of.variance*100, "%")), vjust=-0.3) +
  theme_minimal() + xlab("Dimensions") + ylab("Percentage of explained variances")+
    #title:
  ggtitle("Scree Plot for FA")+ theme(plot.title = element_text(hjust = 0.5)) +
    #text:
  annotate(geom = "text", x = 1, y = 21, label = "Country Development", color = "black",
             angle = 90) +
  annotate(geom = "text", x = 2, y = 10, label = "Quality of Life", color = "black",angle = 90) 
+
    annotate(geom = "text", x = 3, y = 5, label = "Freedom", color = "black",angle = 90)

rm(eig,temp_for_plot,xaxis,temp_df)

```

