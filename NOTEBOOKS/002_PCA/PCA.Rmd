---
title: "PCA"
output: html_document
---

#Principal Component Analysis (PCA):

PCA, known as a dimension reduction tool, is part of the large set of multivariate techniques commonly used. The objective is to reduce our set of original variables into a smaller set of 'components' that still contains most of the variance (information) of our original set of variables. In other words, each principal component is a linear combination of the observed variables. 

```{r, echo = TRUE, fig.width=8, fig.height=8}
#------------------------------------------LOADING---------------------------------------------
require(dplyr);require(FactoMineR);require(factoextra);require(corrplot);require(ggplot2)

# Load dataset into dataframe
full_best_df <- read.csv(file = "../../DATA/full_best_df.csv")
#row.names(full_best_df) <- full_best_df$Country

temp_df <- full_best_df %>% 
  select(-X,-Country,-Region,-Happiness_Score,
         -Total_Population,-Mental_and_Substance_Disorder_Index,-Compulsory_Education_in_Years,
         -Agricultural_Land_Percentage,-Generosity,-Forest_Area_Land_Percentage,
         -Suicide_Index,-Legal_Rights_Index, - Air_Pollution,
         -Infant_Immunization_Measles_Index,-Alcohol_Consumption_Index)
```

First, we are calling the PCA function, which gives a first good overview of correlation biplots on PC1 & PC2. First one being about the variables, while second about the countries.
```{r, echo = TRUE, fig.width=8, fig.height=8}
res.pca <- PCA(temp_df,graph=T) #Biplot of PC1 and PC2
print(res.pca)
```

Scree Plot shows the percentage of explained variance per principal component.
Secondly the contribution of each variable to each component. 
```{r, echo = TRUE, fig.width=8, fig.height=8}
#fviz_eig(res.pca, addlabels=TRUE) #Scree plot
dimdesc(res.pca) #Easier way to see the contribution of each component's variables

#Better visualization of the scree plot with ggplot
xaxis <- 1:10; eig <- data.frame(res.pca$eig); eig <- eig[1:10,]
temp_for_plot <- data.frame(eig,xaxis)

temp_for_plot$percentage.of.variance <- round(temp_for_plot$percentage.of.variance, digits=2)

ggplot(data=temp_for_plot, aes(x=factor(xaxis), y=percentage.of.variance)) +
  geom_bar(stat="identity", fill="steelblue") +
  geom_text(aes(label= paste0(temp_for_plot$percentage.of.variance, "%")), vjust=-0.3) +
  theme_minimal() + xlab("Dimensions") + ylab("Percentage of explained variances")+
  ggtitle("Scree Plot for PCA")+ theme(plot.title = element_text(hjust = 0.5)) +
  scale_colour_manual(values = c("1" = "#E08214", "2" = "#E08214")) +
  annotate(geom = "text", x = 1, y = 30, label = "Country Development", color = "white",
             angle = 90) +
  annotate(geom = "text", x = 2, y = 05, label = "Freedom", color = "white",
             angle = 90)

#res.pca$var$coord #Best way to compare contribution of each variable across several PCs
#res.pca$ind$coord  #Contribution of each country to the PCs
```

Extract the results for variables and individuals, respectively:
```{r, echo = TRUE, fig.width=8, fig.height=8}
var <- get_pca_var(res.pca) #variables
ind <- get_pca_ind(res.pca) #individuals

row.names(res.pca$ind$coord) <- full_best_df$Country
row.names(res.pca$ind$cos2) <- full_best_df$Country

#var/ind$coord -> coordinates
#var/ind$cos2 -> quality of the factored map
#var/ind$contrib -> contributions to the PCs
```

## Quality of Representation (cos2):

Note: High cos2 indicates a good representation, a low one shows the opposite. 

### Of the variables:

```{r, echo = TRUE, fig.width=8, fig.height=8}
corrplot(var$cos2, is.corr=F)
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE) # Avoid text overlapping

# Total cos2 of variables on PC1 and PC2
fviz_cos2(res.pca, choice = "var", axes = 1:2) 

```

### Of the countries:

```{r, echo = TRUE, fig.width=8, fig.height=8}
corrplot(res.pca$ind$cos2[1:45,], is.corr=F)
corrplot(res.pca$ind$cos2[46:91,], is.corr=F)
corrplot(res.pca$ind$cos2[91:132,], is.corr=F)

fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) # Avoid text overlapping (slow if many points)

fviz_cos2(res.pca, choice = "ind", top=40) #Quality of representation graph
```

# Classification into X groups using Kmeans Clustering Algo:

First, trying the algo with a simple clustering method:

```{r, echo = TRUE, fig.width=8, fig.height=8}

# Create a grouping variable using kmeans
# Create 3 groups of variables (centers = 3)
set.seed(123)
res.km <- kmeans(var$coord, centers = 3, nstart = 25)
grp <- as.factor(res.km$cluster)
# Color variables by groups
fviz_pca_var(res.pca, col.var = grp, 
             palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
             legend.title = "Cluster", repel=TRUE)


# Create a grouping of countries using kmeans
# Create 3 groups of countries (centers = 3)
set.seed(123)
res.km <- kmeans(ind$coord, centers = 3, nstart = 25)
grp <- as.factor(res.km$cluster)
# Color countries by groups
fviz_pca_ind(res.pca, col.ind = grp, 
             palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
             legend.title = "Cluster",repel=TRUE)
```

Now trying the same algo, grouping the countries by their happiness score. 
As the Happiness_Score is a continous variable, let see the results when factorising into 3,4,5 groups. 

```{r, echo = TRUE, fig.width=8, fig.height=8}
target_Factorised <- full_best_df

target_Factorised$Happiness_Score <- cut(target_Factorised$Happiness_Score,breaks=4)
fviz_pca_ind(res.pca,
             geom.ind = "text", # show points only (nbut not "text")
             col.ind = target_Factorised$Happiness_Score, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07", "blue"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups", repel=TRUE)

###REGION CLUSTER GROUPS ON PCA
fviz_pca_ind(res.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = full_best_df$Region, # color by groups
             palette = c("blue","red","yellow","black","brown",
                         "purple","green","orange","darkgray","darkgoldenrod4"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups")


```

Trying to visualize on the same biplot variables&countries:
```{r, echo = TRUE, fig.width=8, fig.height=8}
# bad bad visual
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969")  # Individuals color
```

## 3D Plot Visualization:

3D Plot visualisation test (not good):
```{r, echo = TRUE, fig.width=8, fig.height=8}
require(plot3D)

a <- res.pca$ind$coord[,1]
b <- res.pca$ind$coord[,2]
c <- res.pca$ind$coord[,3]

data = data.frame(a,b,c)
scatter3D(b,c,a)
```

# Linear Regression on Happiness Score with the PCs:

## Simple Linear Regression (Only with PC1)

Simple Linear Regression using only the 1st Principal Component:
```{r, echo = TRUE, fig.width=8, fig.height=8}

singlelm <- lm(full_best_df$Happiness_Score ~ res.pca$ind$coord[,1])
summary(singlelm)
plot(res.pca$ind$coord[,1],full_best_df$Happiness_Score)
abline(singlelm)
#text(res.pca$ind$coord[,1],full_best_df$Happiness_Score, label=full_best_df$Country)
```

## Multiple Linear Regression (with 3pcs or 2pcs):

MLR with a combination of 3pcs:
```{r, echo = TRUE, fig.width=8, fig.height=8}
require(Hmisc);require(ggplot2);require(rgl)

predictors <- data.frame(res.pca$ind$coord[,1:3],happiness_score = full_best_df$Happiness_Score)
fit3pcs <- lm(happiness_score~ Dim.1 + Dim.2 + Dim.3, data=predictors)
summary(fit3pcs)

#Another way to do the same...
#pccc <- res.pca$ind$coord[,1:3] 
#threepcfit  <- lm(full_best_df$Happiness_Score ~ pccc)
#summary(threepcfit)
```

MLR with PC1 + PC2 then PC1 + PC3:
```{r, echo = TRUE, fig.width=8, fig.height=8}

fit2firstpcs <- lm(happiness_score~ Dim.1 + Dim.2, data=predictors)
summary(fit2firstpcs)

fitfirstthirdpcs <- lm(happiness_score~ Dim.1 + Dim.3, data=predictors)
summary(fitfirstthirdpcs)
```

Split df into training and validation:
```{r, echo = TRUE, fig.width=8, fig.height=8}
#Split data into training and validation samples 
set.seed(2018)
train.size <- 0.5
train.index <- sample.int(length(full_best_df$Happiness_Score),round(length(full_best_df$Happiness_Score) * train.size))
train.sample <- selection_df[train.index,]
validation.sample <- selection_df[-train.index,]
```

# Regression Trees - CART on Happiness_Score:

```{r, echo = TRUE, fig.width=8, fig.height=8}
require(rpart)
require(rpart.plot)

m1 <-rpart(Happiness_Score~ ., data=train.sample, method= "anova")
m1
rpart.plot(m1, type=3)

p1 <- predict(m1,validation.sample) #prediction values
p1

#let see how accurate the algorithm is
## mean absolute error way to see, (compare the predictions to the actual values)

assess <- function(actual, predicted) { mean(abs(actual - predicted))}

assess(selection_df$Happiness_Score, p1) 
```


